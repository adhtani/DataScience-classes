{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow.json import read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_data(src_data_path):\n",
    "    with open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data_path = 'C:/Users/bibek/Documents/GitHub/dsc650/data/processed/openflights/routes.jsonl.gz'\n",
    "records = read_jsonl_data(src_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genson\n",
    "from genson import SchemaBuilder\n",
    "\n",
    "builder = SchemaBuilder()\n",
    "builder.add_object(records[0])\n",
    "schema = builder.to_schema()\n",
    "\n",
    "schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "with open(schema_path, \"w\") as f:\n",
    "    json.dump(schema, f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from jsonschema import validate\n",
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    validation_csv_path = results_dir.joinpath('validation-results.csv')\n",
    "    fields = [\"record Number\", \"Validity\"]\n",
    "    with open(validation_csv_path, 'w', newline='') as f:\n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(f) \n",
    "        \n",
    "        # writing the fields \n",
    "        csvwriter.writerow(fields)\n",
    "        row = []\n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                validate(instance=record, schema=schema)\n",
    "                row.append([i, \"valid\"])\n",
    "                pass\n",
    "            except ValidationError as e:\n",
    "                ## Print message if invalid record in csv file\n",
    "                row.append([i, \"invalid\"])\n",
    "                pass\n",
    "         \n",
    "        csvwriter.writerows(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastavro import writer, reader, parse_schema\n",
    "from fastavro.schema import load_schema\n",
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    schema = load_schema(schema_path)\n",
    "    \n",
    "    # Writing\n",
    "    with open(data_path, 'wb') as out:\n",
    "        writer(out, schema, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset(src_data_path):  \n",
    "    parquet_path = results_dir.joinpath('routes.parquet')\n",
    "    with open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            pass\n",
    "            table = read_json(f) \n",
    "            pq.write_table(table, parquet_path)  # save json/table as parquet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parquet_dataset(src_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "\n",
    "    if airline is None:\n",
    "        return obj\n",
    "    if airline.get(\"airline_id\") is None:\n",
    "        return obj\n",
    "    \n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    if airline.get('name'):\n",
    "        obj.name = airline.get('name')\n",
    "    if airline.get('alias'):\n",
    "        obj.alias = airline.get('alias')\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('callsign'):\n",
    "        obj.callsign = airline.get('callsign')\n",
    "    if airline.get('country'):\n",
    "        obj.country = airline.get('country')\n",
    "    \n",
    "    obj.active = airline.get('active')\n",
    "        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        route.codeshare = record.get('codeshare') #required from .proto file\n",
    "        \n",
    "        airline = _airline_to_proto_obj(record.get('airline'))\n",
    "        if airline:\n",
    "            route.airline.CopyFrom(airline)\n",
    "        \n",
    "        src_airport = _airport_to_proto_obj(record.get('src_airport'))\n",
    "        if src_airport:\n",
    "            route.src_airport.CopyFrom(src_airport)\n",
    "            \n",
    "        dst_airport = _airport_to_proto_obj(record.get('dst_airport'))\n",
    "        if dst_airport:\n",
    "            route.dst_airport.CopyFrom(dst_airport)\n",
    "            \n",
    "        equipment = record.get('equipment')\n",
    "        route.equipment.extend(equipment)     \n",
    "        \n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "    \n",
    "    #compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    #with open(compressed_path, 'wb') as f:\n",
    "        #f.write(snappy.compress(routes.SerializeToString()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.e Output Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'routes.avro' Size is : 18.73610210418701 MB\n",
      "'routes.parquet' Size is : 1.8192329406738281 MB\n",
      "'routes.pb' Size is : 21.479753494262695 MB\n",
      "'validation-results.csv' Size is : 0.8299989700317383 MB\n"
     ]
    }
   ],
   "source": [
    "comparison_csv_path = results_dir.joinpath('comparison.csv')\n",
    "\n",
    "files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(results_dir):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "files.pop(0)\n",
    "\n",
    "fields = ['file_name', 'size_MB']\n",
    "with open(comparison_csv_path, 'w', newline='') as file:\n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(file) \n",
    "    # writing the fields \n",
    "    csvwriter.writerow(fields)\n",
    "    row = []\n",
    "    for i in range(len(files)):\n",
    "        path = results_dir.joinpath(files[i])\n",
    "        file_size = os.path.getsize(path)/(1024*1024)\n",
    "        print(\"'{}' Size is : {} MB\".format(files[i],file_size))\n",
    "        row.append([files[i], \"{} MB\".format(file_size)])\n",
    "    \n",
    "    csvwriter.writerows(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    hashes = []\n",
    "    for record in records:\n",
    "        \n",
    "        #getting src airport from records\n",
    "        src_airport = record.get('src_airport')\n",
    "        if src_airport:\n",
    "            latitude = src_airport.get('latitude')\n",
    "            longitude = src_airport.get('longitude')\n",
    "            if latitude and longitude:\n",
    "                \n",
    "                # use pygeohash.encode() to assign geohashes to the records and complete the hashes list\n",
    "                geo_hash = pygeohash.encode(latitude, longitude)\n",
    "                hashes.append(geo_hash)\n",
    "                i = records.index(record)\n",
    "                records[i]['geohash'] = geo_hash #assigning geohash to each record\n",
    "                   \n",
    "    \n",
    "    hashes.sort()\n",
    "    three_letter = sorted(list(set([entry[:3] for entry in hashes])))\n",
    "    \n",
    "    # dictionary with geohash \"first\" 3 letters and record as a value\n",
    "    hash_index = {value: [] for value in three_letter} \n",
    "    for record in records:\n",
    "        geohash = record.get('geohash')\n",
    "        if geohash:\n",
    "            hash_index[geohash[:3]].append(record)\n",
    "    \n",
    "    for key, values in hash_index.items():\n",
    "        #creating zip files for geohashes\n",
    "        output_dir = geoindex_dir.joinpath(str(key[:1])).joinpath(str(key[:2]))\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        output_path = output_dir.joinpath('{}.jsonl.gz'.format(key))\n",
    "        with gzip.open(output_path, 'w') as f:\n",
    "            json_output = '\\n'.join([json.dumps(value) for value in values])\n",
    "            f.write(json_output.encode('utf-8'))\n",
    "            \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash\n",
    "def airport_search(latitude, longitude):\n",
    "    geohash = pygeohash.encode(latitude, longitude)\n",
    "    try:\n",
    "        search_dir = results_dir.joinpath('geoindex').joinpath(geohash[0]).joinpath(geohash[0:2])\n",
    "        filename = search_dir.joinpath('{}.jsonl.gz'.format(geohash[:3]))\n",
    "        records = read_jsonl_data(filename)\n",
    "        Airport = {}\n",
    "        UniqueHash = [] #uniquehash list for optimal solution\n",
    "        Distance = [] #list for all nearest airports\n",
    "        index_dist = {} # dictionary for record index and distance of airport\n",
    "        for record in records:\n",
    "            i = records.index(record)\n",
    "            hash1 = records[i]['geohash']\n",
    "            if hash1[:3] == geohash[:3]:\n",
    "                if hash1 not in UniqueHash:\n",
    "                    UniqueHash.append(hash1)\n",
    "                    d = pygeohash.geohash_approximate_distance(hash1, geohash)/1000\n",
    "                    index_dist[i] = d\n",
    "                    Distance.append(d)\n",
    "        \n",
    "        for k, v in index_dist.items():\n",
    "            if v == min(Distance): #finds minimum distance\n",
    "                name = records[int(k)]['src_airport']['name']\n",
    "                Airport[name] = '{} km'.format(v)\n",
    "        print(\"Nearest Airport: {}\".format(Airport))\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Airports Not Found\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Airport: {'Eppley Airfield': '19.545 km'}\n"
     ]
    }
   ],
   "source": [
    "airport_search(41.1499988, -95.91779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Airport: {'Manhattan Regional Airport': '123.264 km', 'Topeka Regional Airport - Forbes Field': '123.264 km'}\n"
     ]
    }
   ],
   "source": [
    "airport_search(38.5565, -95.91779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Airport: {'Dallas Love Field': '123.264 km'}\n"
     ]
    }
   ],
   "source": [
    "airport_search(32.6549, -95.91779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Airport: {'Wichita Eisenhower National Airport': '123.264 km'}\n"
     ]
    }
   ],
   "source": [
    "airport_search(36.6549, -97.91779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
