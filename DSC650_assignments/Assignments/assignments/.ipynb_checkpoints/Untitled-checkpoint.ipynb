{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "3.6 Predicting house prices: a regression exampleÂ¶\n",
    "3.6.1 The Boston Housing Price dataset\n",
    "In [1]:\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Importing the data to training and testing sets\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "In [2]:\n",
    "# Checking the shape of the data\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "(404, 13)\n",
    "(102, 13)\n",
    "In [3]:\n",
    "# Targets are home prices in thousands of dollars (from the 1970s)\n",
    "train_targets[:3]\n",
    "Out[3]:\n",
    "array([15.2, 42.3, 50. ])\n",
    "3.6.2 Preparing the data\n",
    "In [4]:\n",
    "# Normalizing the data for the neural network\n",
    "mean = train_data.mean(axis = 0)\n",
    "train_data -= mean\n",
    "\n",
    "std = train_data.std(axis = 0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std\n",
    "3.6.3 Building your network\n",
    "In [5]:\n",
    "from keras import models, layers\n",
    "In [6]:\n",
    "# Building a small network to prevent overfitting\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
    "    return model\n",
    "3.6.4 Validation your approach using K-folds validation\n",
    "In [7]:\n",
    "import numpy as np\n",
    "\n",
    "# Perform a test of k-fold cross validation\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print(f'Processing Fold #{i+1}')\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data  = np.concatenate(\n",
    "        [train_data[:i * num_val_samples], \n",
    "         train_data[(i + 1) * num_val_samples:]], \n",
    "        axis = 0 )\n",
    "    \n",
    "    partial_train_targets  = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples], \n",
    "         train_targets[(i + 1) * num_val_samples:]], \n",
    "        axis = 0)\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    model.fit(partial_train_data, partial_train_targets, \n",
    "              epochs = num_epochs, batch_size = 1, verbose=False)\n",
    "    \n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=False)\n",
    "    \n",
    "    all_scores.append(val_mae)\n",
    "Processing Fold #1\n",
    "Processing Fold #2\n",
    "Processing Fold #3\n",
    "Processing Fold #4\n",
    "In [8]:\n",
    "all_scores\n",
    "Out[8]:\n",
    "[1.9999395608901978, 2.9141619205474854, 2.5237762928009033, 2.259720802307129]\n",
    "In [9]:\n",
    "np.mean(all_scores)\n",
    "Out[9]:\n",
    "2.424399644136429\n",
    "In [10]:\n",
    "# Perform a 500 epoch of k-fold cross validation\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print(f'Processing Fold #{i+1}')\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    \n",
    "    model = build_model()\n",
    "    \n",
    "    history = model.fit(partial_train_data,\n",
    "                       partial_train_targets,\n",
    "                       validation_data=(val_data,val_targets),\n",
    "                       epochs=num_epochs,\n",
    "                       batch_size=1,\n",
    "                       verbose=0)\n",
    "    \n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories.append(mae_history)\n",
    "Processing Fold #1\n",
    "Processing Fold #2\n",
    "Processing Fold #3\n",
    "Processing Fold #4\n",
    "In [11]:\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "In [12]:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "In [13]:\n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()\n",
    "\n",
    "In [14]:\n",
    "model = build_model()\n",
    "model.fit(train_data,\n",
    "         train_targets,\n",
    "         epochs=80,\n",
    "         batch_size=16,\n",
    "         verbose=0)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "4/4 [==============================] - 1s 4ms/step - loss: 19.2182 - mae: 2.7113\n",
    "In [15]:\n",
    "test_mae_score\n",
    "Out[15]:\n",
    "2.7113044261932373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Assignment 6.1\n",
    "Using section 5.1 in Deep Learning with Python as a guide (listing 5.3 in particular), create a ConvNet model that classifies images in the MNIST digit dataset. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.\n",
    "\n",
    "In [1]:\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "Data Exploration\n",
    "In [2]:\n",
    "# Loading the data into memory\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "fig, [[ax0, ax1, ax2, ax3, ax4], \n",
    "      [ax5, ax6, ax7, ax8, ax9]] = plt.subplots(2,5, figsize=(16,9))\n",
    "\n",
    "ax0.imshow(train_images[1], cmap='gray')\n",
    "ax1.imshow(train_images[3], cmap='gray')\n",
    "ax2.imshow(train_images[5], cmap='gray')\n",
    "ax3.imshow(train_images[7], cmap='gray')\n",
    "ax4.imshow(train_images[2], cmap='gray')\n",
    "ax5.imshow(train_images[11], cmap='gray')\n",
    "ax6.imshow(train_images[18], cmap='gray')\n",
    "ax7.imshow(train_images[29], cmap='gray')\n",
    "ax8.imshow(train_images[17], cmap='gray')\n",
    "ax9.imshow(train_images[4], cmap='gray')\n",
    "Out[2]:\n",
    "<matplotlib.image.AxesImage at 0x1fc95985460>\n",
    "\n",
    "The training data contains 60,000 observations of 28x28 pixel images with targets ranging from 0 to 9\n",
    "\n",
    "In [3]:\n",
    "train_images.shape\n",
    "Out[3]:\n",
    "(60000, 28, 28)\n",
    "In [4]:\n",
    "set(train_labels)\n",
    "Out[4]:\n",
    "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "The testing set contains 10,000 images\n",
    "\n",
    "In [5]:\n",
    "test_images.shape\n",
    "Out[5]:\n",
    "(10000, 28, 28)\n",
    "The images contain gray level values ranging from 0 (black) to 255 (white)\n",
    "\n",
    "In [6]:\n",
    "train_images[0].max()\n",
    "Out[6]:\n",
    "255\n",
    "Preparing the data\n",
    "The data will need to be reshaped and normalized before we feed it into a neural network\n",
    "\n",
    "In [7]:\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images / train_images.max()\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images / train_images.max()\n",
    "\n",
    "train_images_val = train_images[:10000]\n",
    "train_images = train_images[10000:]\n",
    "\n",
    "train_labels_val = train_labels[:10000]\n",
    "train_labels = train_labels[10000:]\n",
    "Building the neural net\n",
    "In [8]:\n",
    "from keras import models\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "In [9]:\n",
    "model = models.Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "In [10]:\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=64, \n",
    "                    validation_data = (train_images_val, train_labels_val), \n",
    "                    verbose=True)\n",
    "Epoch 1/20\n",
    "782/782 [==============================] - 34s 42ms/step - loss: 0.4694 - acc: 0.8456 - val_loss: 0.1131 - val_acc: 0.9671\n",
    "Epoch 2/20\n",
    "782/782 [==============================] - 33s 42ms/step - loss: 0.0537 - acc: 0.9832 - val_loss: 0.0443 - val_acc: 0.9881\n",
    "Epoch 3/20\n",
    "782/782 [==============================] - 30s 39ms/step - loss: 0.0351 - acc: 0.9890 - val_loss: 0.0438 - val_acc: 0.9886\n",
    "Epoch 4/20\n",
    "782/782 [==============================] - 31s 40ms/step - loss: 0.0254 - acc: 0.9921 - val_loss: 0.0376 - val_acc: 0.9890\n",
    "Epoch 5/20\n",
    "782/782 [==============================] - 32s 41ms/step - loss: 0.0207 - acc: 0.9936 - val_loss: 0.0533 - val_acc: 0.9882\n",
    "Epoch 6/20\n",
    "782/782 [==============================] - 32s 41ms/step - loss: 0.0154 - acc: 0.9955 - val_loss: 0.0458 - val_acc: 0.9900\n",
    "Epoch 7/20\n",
    "782/782 [==============================] - 36s 46ms/step - loss: 0.0110 - acc: 0.9966 - val_loss: 0.0407 - val_acc: 0.9898\n",
    "Epoch 8/20\n",
    "782/782 [==============================] - 34s 43ms/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0459 - val_acc: 0.9910\n",
    "Epoch 9/20\n",
    "782/782 [==============================] - 32s 40ms/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0521 - val_acc: 0.9910\n",
    "Epoch 10/20\n",
    "782/782 [==============================] - 33s 42ms/step - loss: 0.0065 - acc: 0.9979 - val_loss: 0.0558 - val_acc: 0.9889\n",
    "Epoch 11/20\n",
    "782/782 [==============================] - 33s 42ms/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0644 - val_acc: 0.9886\n",
    "Epoch 12/20\n",
    "782/782 [==============================] - 33s 42ms/step - loss: 0.0052 - acc: 0.9981 - val_loss: 0.0589 - val_acc: 0.9906\n",
    "Epoch 13/20\n",
    "782/782 [==============================] - 33s 42ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0729 - val_acc: 0.9905\n",
    "Epoch 14/20\n",
    "782/782 [==============================] - 32s 41ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0612 - val_acc: 0.9909\n",
    "Epoch 15/20\n",
    "782/782 [==============================] - 32s 40ms/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0742 - val_acc: 0.9903\n",
    "Epoch 16/20\n",
    "782/782 [==============================] - 33s 42ms/step - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0823 - val_acc: 0.9902\n",
    "Epoch 17/20\n",
    "782/782 [==============================] - 32s 41ms/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0637 - val_acc: 0.9917\n",
    "Epoch 18/20\n",
    "782/782 [==============================] - 33s 43ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0852 - val_acc: 0.9909\n",
    "Epoch 19/20\n",
    "782/782 [==============================] - 33s 42ms/step - loss: 0.0026 - acc: 0.9994 - val_loss: 0.0752 - val_acc: 0.9905\n",
    "Epoch 20/20\n",
    "782/782 [==============================] - 33s 43ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0815 - val_acc: 0.9914\n",
    "In [11]:\n",
    "import sklearn.metrics as metrics\n",
    "from seaborn import heatmap\n",
    "import numpy as np\n",
    "In [12]:\n",
    "results = model.evaluate(test_images, test_labels)\n",
    "print(results)\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1,len(acc) + 1)\n",
    "\n",
    "# Plotting metrics\n",
    "fig, [ax1, ax2, ax3] = plt.subplots(1,3, figsize=(16,4))\n",
    "\n",
    "ax1.plot(epochs, loss_values,  'bo', label = 'Training Loss')\n",
    "ax1.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(epochs, acc,  'bo', label = 'Training accuracy')\n",
    "ax2.plot(epochs, val_acc, 'b', label = 'Validation accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=test_labels, \n",
    "                                            y_pred=model.predict(test_images).argmax(axis = 1))\n",
    "heatmap(confusion_matrix, annot = True, cmap='Blues', fmt='g', ax = ax3);\n",
    "313/313 [==============================] - 2s 7ms/step - loss: 25.2807 - acc: 0.9882\n",
    "[25.280717849731445, 0.9882000088691711]\n",
    "\n",
    "In [13]:\n",
    "model = models.Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=5,\n",
    "                    batch_size=64, \n",
    "                    validation_data = (train_images_val, train_labels_val))\n",
    "Epoch 1/5\n",
    "782/782 [==============================] - 33s 41ms/step - loss: 0.4356 - accuracy: 0.8596 - val_loss: 0.0683 - val_accuracy: 0.9792\n",
    "Epoch 2/5\n",
    "782/782 [==============================] - 31s 40ms/step - loss: 0.0496 - accuracy: 0.9852 - val_loss: 0.0545 - val_accuracy: 0.9855\n",
    "Epoch 3/5\n",
    "782/782 [==============================] - 32s 41ms/step - loss: 0.0337 - accuracy: 0.9887 - val_loss: 0.0387 - val_accuracy: 0.9885\n",
    "Epoch 4/5\n",
    "782/782 [==============================] - 31s 39ms/step - loss: 0.0276 - accuracy: 0.9916 - val_loss: 0.0448 - val_accuracy: 0.9871\n",
    "Epoch 5/5\n",
    "782/782 [==============================] - 31s 40ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.0449 - val_accuracy: 0.9892\n",
    "In [14]:\n",
    "results = model.evaluate(test_images, test_labels)\n",
    "print(results)\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1,len(acc) + 1)\n",
    "\n",
    "# Plotting metrics\n",
    "fig, [ax1, ax2, ax3] = plt.subplots(1,3, figsize=(16,4))\n",
    "\n",
    "ax1.plot(epochs, loss_values,  'bo', label = 'Training Loss')\n",
    "ax1.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(epochs, acc,  'bo', label = 'Training accuracy')\n",
    "ax2.plot(epochs, val_acc, 'b', label = 'Validation accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=test_labels, \n",
    "                                            y_pred=model.predict(test_images).argmax(axis = 1))\n",
    "heatmap(confusion_matrix, annot = True, cmap='Blues', fmt='g', ax = ax3);\n",
    "313/313 [==============================] - 2s 6ms/step - loss: 11.5415 - accuracy: 0.9860\n",
    "[11.541476249694824, 0.9860000014305115]\n",
    "\n",
    "Save Results\n",
    "In [15]:\n",
    "model.save('results/model_6_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Assignment 6.2Â¶\n",
    "Assignment 6.2.a\n",
    "Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset. Do not use dropout or data-augmentation in this part. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.\n",
    "\n",
    "In [1]:\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from seaborn import heatmap\n",
    "In [2]:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "Data Exploration\n",
    "In [3]:\n",
    "axs = [f'ax{i}' for i in range(0,10)]\n",
    "\n",
    "fig, [axs[0:5], \n",
    "      axs[5:10]] = plt.subplots(2,5, figsize=(16,9))\n",
    "\n",
    "for ax in axs:\n",
    "    ax.imshow(X_train[np.random.randint(0,10000)])\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "In [4]:\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "X_train shape: (50000, 32, 32, 3)\n",
    "y_train shape: (50000, 1)\n",
    "50000 train samples\n",
    "10000 test samples\n",
    "Data Preprocessing\n",
    "In [5]:\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "\n",
    "X_val_train = X_train[:10000]\n",
    "X_train = X_train[10000:]\n",
    "\n",
    "# Convert target data to single array of shape (50000,) and (10000,)\n",
    "y_train = y_train.reshape(y_train.shape[0])\n",
    "y_test = y_test.reshape(y_test.shape[0])\n",
    "\n",
    "y_val_train = y_train[:10000]\n",
    "y_train = y_train[10000:]\n",
    "ConvNet Model\n",
    "In [6]:\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "In [7]:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dropout, Dense, Flatten\n",
    "In [8]:\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), \n",
    "                 padding='same', \n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
    "_________________________________________________________________\n",
    "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 5, 5, 128)         73856     \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 128)         0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                5130      \n",
    "=================================================================\n",
    "Total params: 361,034\n",
    "Trainable params: 361,034\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "In [9]:\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),\n",
    "              metrics=['acc'])\n",
    "In [10]:\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs = epochs, \n",
    "                    validation_data=(X_val_train, y_val_train), \n",
    "                    verbose=False)\n",
    "In [11]:\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(results)\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1,len(acc) + 1)\n",
    "\n",
    "# Plotting metrics\n",
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "ax1.plot(epochs, loss_values,  'k', label = 'Training Loss')\n",
    "ax1.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(epochs, acc,  'k', label = 'Training accuracy')\n",
    "ax2.plot(epochs, val_acc, 'b', label = 'Validation accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "313/313 [==============================] - 2s 5ms/step - loss: 2.3716 - acc: 0.6730\n",
    "[2.371561050415039, 0.6729999780654907]\n",
    "\n",
    "In [12]:\n",
    "batch_size = 128\n",
    "epochs = 60\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), \n",
    "                 padding='same', \n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs = epochs, \n",
    "                    validation_data=(X_val_train, y_val_train), \n",
    "                    verbose=False)\n",
    "In [13]:\n",
    "predictions = model.predict(X_test)\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=predictions.argmax(axis=1))\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "heatmap(confusion_matrix, annot = True, cmap='Blues', fmt='g', ax = ax);\n",
    "\n",
    "In [14]:\n",
    "predictions.argmax(axis = 1).shape\n",
    "Out[14]:\n",
    "(10000,)\n",
    "In [15]:\n",
    "labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['prediction'] = predictions.argmax(axis=1)\n",
    "df['actual'] = y_test\n",
    "df['prediction'] = df['prediction'].apply(lambda x: labels[x])\n",
    "df['actual'] = df['actual'].apply(lambda x: labels[x])\n",
    "df.to_csv(\"results/model_6_a_predictions.csv\")\n",
    "df.sample(20)\n",
    "Out[15]:\n",
    "prediction\tactual\n",
    "1384\tTruck\tTruck\n",
    "1264\tShip\tShip\n",
    "7653\tShip\tShip\n",
    "2146\tBird\tBird\n",
    "4286\tBird\tBird\n",
    "3676\tTruck\tTruck\n",
    "8386\tCat\tCat\n",
    "7997\tTruck\tAutomobile\n",
    "2335\tDeer\tDog\n",
    "2559\tHorse\tHorse\n",
    "3497\tAirplane\tAirplane\n",
    "8016\tDeer\tDeer\n",
    "1939\tHorse\tFrog\n",
    "935\tBird\tBird\n",
    "465\tDeer\tDeer\n",
    "3646\tCat\tCat\n",
    "3160\tFrog\tFrog\n",
    "2767\tHorse\tDog\n",
    "9019\tTruck\tTruck\n",
    "3047\tCat\tFrog\n",
    "In [16]:\n",
    "model.save('results/model_6_2_a.h5')\n",
    "Assignment 6.2.b\n",
    "Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset. This time includes dropout and data-augmentation. Save the model, predictions, metrics, and validation plots in the dsc650/assignments/assignment06/results directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.\n",
    "\n",
    "In [17]:\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "Model: \"sequential_2\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_6 (Conv2D)            (None, 32, 32, 32)        896       \n",
    "_________________________________________________________________\n",
    "activation (Activation)      (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_7 (Conv2D)            (None, 30, 30, 32)        9248      \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 30, 30, 32)        0         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         \n",
    "_________________________________________________________________\n",
    "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_8 (Conv2D)            (None, 15, 15, 64)        18496     \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 15, 15, 64)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_9 (Conv2D)            (None, 13, 13, 64)        36928     \n",
    "_________________________________________________________________\n",
    "activation_3 (Activation)    (None, 13, 13, 64)        0         \n",
    "_________________________________________________________________\n",
    "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 64)          0         \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
    "_________________________________________________________________\n",
    "flatten_2 (Flatten)          (None, 2304)              0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 512)               1180160   \n",
    "_________________________________________________________________\n",
    "activation_4 (Activation)    (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_5 (Dense)              (None, 10)                5130      \n",
    "_________________________________________________________________\n",
    "activation_5 (Activation)    (None, 10)                0         \n",
    "=================================================================\n",
    "Total params: 1,250,858\n",
    "Trainable params: 1,250,858\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "In [18]:\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6),\n",
    "              metrics=['acc'])\n",
    "In [19]:\n",
    "batch_size = 128\n",
    "epochs = 300\n",
    "In [20]:\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "history = model.fit(datagen.flow(X_train, \n",
    "                                 y_train,\n",
    "                                 batch_size=batch_size),\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_val_train, y_val_train),\n",
    "                    workers=4, \n",
    "                    verbose=False)\n",
    "In [21]:\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(results)\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1,len(acc) + 1)\n",
    "\n",
    "# Plotting metrics\n",
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize=(16,8))\n",
    "\n",
    "ax1.plot(epochs, loss_values,  'k', label = 'Training Loss')\n",
    "ax1.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(epochs, acc,  'k', label = 'Training accuracy')\n",
    "ax2.plot(epochs, val_acc, 'b', label = 'Validation accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "313/313 [==============================] - 2s 8ms/step - loss: 0.8179 - acc: 0.7187\n",
    "[0.8179059028625488, 0.7186999917030334]\n",
    "\n",
    "In [22]:\n",
    "predictions = model.predict(X_test)\n",
    "confusion_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=predictions.argmax(axis=1))\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "heatmap(confusion_matrix, annot = True, cmap='Blues', fmt='g', ax = ax);\n",
    "\n",
    "In [23]:\n",
    "labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['prediction'] = predictions.argmax(axis=1)\n",
    "df['actual'] = y_test\n",
    "df['prediction'] = df['prediction'].apply(lambda x: labels[x])\n",
    "df['actual'] = df['actual'].apply(lambda x: labels[x])\n",
    "df.to_csv(\"results/model_6_b_predictions.csv\")\n",
    "df.sample(20)\n",
    "Out[23]:\n",
    "prediction\tactual\n",
    "9811\tAutomobile\tAutomobile\n",
    "2368\tAirplane\tAirplane\n",
    "9518\tTruck\tTruck\n",
    "4599\tAutomobile\tShip\n",
    "2648\tDeer\tDeer\n",
    "1234\tAutomobile\tAutomobile\n",
    "8059\tCat\tCat\n",
    "780\tTruck\tTruck\n",
    "6877\tShip\tAirplane\n",
    "131\tAutomobile\tAutomobile\n",
    "9711\tDeer\tDeer\n",
    "2102\tAirplane\tAirplane\n",
    "2337\tHorse\tHorse\n",
    "9285\tShip\tShip\n",
    "4948\tAutomobile\tAutomobile\n",
    "7863\tAirplane\tAirplane\n",
    "5954\tShip\tShip\n",
    "2398\tShip\tShip\n",
    "9723\tHorse\tHorse\n",
    "2805\tDeer\tDeer\n",
    "In [24]:\n",
    "model.save('results/model_6_2_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Assignment 6.3Â¶\n",
    "Load the ResNet50 model. Perform image classification on five to ten images of your choice. They can be personal images or publically available images. Include the images in dsc650/assignments/assignment06/images/. Save the predictions dsc650/assignments/assignment06/results/predictions/resnet50 directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.\n",
    "\n",
    "In [1]:\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "def process_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return(img)\n",
    "\n",
    "def predict_image(processed_img):\n",
    "    preds = model.predict(processed_img)\n",
    "    prediction = decode_predictions(preds, top=1)[0][0]\n",
    "    _, description, probability = prediction\n",
    "    return description, probability\n",
    "In [2]:\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "images_dir = current_dir.joinpath('images')\n",
    "\n",
    "for root, dirs, pictures in os.walk(images_dir):\n",
    "    for picture in pictures:\n",
    "        image_path = Path(root).joinpath(picture)\n",
    "        img = process_image(image_path)\n",
    "        description, probability = predict_image(img)\n",
    "        pic = mpimg.imread(image_path)\n",
    "        plt.imshow(pic)\n",
    "        plt.title(f'{picture}\\nPrediction: {description:^}\\nProbability: {probability:.3f}')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In [ ]:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
